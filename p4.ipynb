{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9cc87b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean, Standard Deviation, Medians, Betas, Phos\n",
      "50\n",
      "3.830255549440328\n"
     ]
    }
   ],
   "source": [
    "## Written by: WAI ZIN LINN\n",
    "## Attribution: Hugh Liu's solutions for CS540 2021 Epic and Hongtao Hao\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "# modify the following parameters if yours are different\n",
    "num_of_parameters = 5\n",
    "target_num_clusters = 7\n",
    "target_states = ['Wisconsin', 'Alabama']\n",
    "\n",
    "df = pd.read_csv('time_series_covid19_deaths_US.csv')\n",
    "all_states = list(set(df.Province_State))\n",
    "\n",
    "# sort in alphabet order\n",
    "all_states.sort()\n",
    "to_remove_states = ['Grand Princess', \n",
    "                    'Diamond Princess', \n",
    "                    'Guam', \n",
    "                    'American Samoa', \n",
    "                    'Virgin Islands',\n",
    "                    'Northern Mariana Islands',\n",
    "                    'Puerto Rico',\n",
    "                    'District of Columbia',\n",
    "                   ]\n",
    "all_states = [x for x in all_states if x not in to_remove_states]\n",
    "num_of_states = len(all_states)\n",
    "\n",
    "def get_cumulative_timeseries(df, target_states):\n",
    "    '''This function returns the cumulative timeseries death data for target states or all states\n",
    "    Input:\n",
    "        df: df\n",
    "        target_states: e.g., ['Wisconsin', 'Alabama'], or all states\n",
    "    '''\n",
    "    cumulative_timeseries_data_list = []\n",
    "    first_date_col = df.columns.get_loc(\"1/22/20\")\n",
    "    for state in target_states:\n",
    "        state_df = df[df.Province_State == state]\n",
    "        # death in a million\n",
    "        state_population = state_df['Population'].sum()/10**6\n",
    "        if state_population == 0:\n",
    "            state_population = 1\n",
    "        state_timeseries = state_df.iloc[:, first_date_col:]\n",
    "        if target_states == all_states:\n",
    "            state_cumulative_timeseries = (state_timeseries.sum(axis = 0)/state_population).tolist()\n",
    "        else:\n",
    "            state_cumulative_timeseries = (state_timeseries.sum(axis = 0)).tolist()\n",
    "        cumulative_timeseries_data_list.append(state_cumulative_timeseries)\n",
    "    return cumulative_timeseries_data_list\n",
    "\n",
    "def get_time_diff(cumulative_timeseries_data_list):\n",
    "    '''This function returns the timeseries differnece data\n",
    "    Input:\n",
    "        a list of cumulative timeseries data\n",
    "    Return:\n",
    "        a list of numpy arrays\n",
    "    '''\n",
    "    time_diff_list = []\n",
    "    for state_cum_ts in cumulative_timeseries_data_list:\n",
    "        state_time_diff = []\n",
    "        for i in range(len(state_cum_ts)-1):\n",
    "            state_time_diff.append(state_cum_ts[i+1] - state_cum_ts[i])\n",
    "        time_diff_list.append(np.array(state_time_diff))\n",
    "    return time_diff_list\n",
    "\n",
    "##q1\n",
    "cumulative_timeseries_data_list = get_cumulative_timeseries(df, target_states)\n",
    "# wisconsin\n",
    "wi_cum_ts = cumulative_timeseries_data_list[0]\n",
    "# alabama\n",
    "al_cum_ts = cumulative_timeseries_data_list[1]\n",
    "\n",
    "with open('q1.txt', 'w') as f:\n",
    "    for wi in wi_cum_ts:\n",
    "        f.write(str(wi) + \",\")\n",
    "    f.write(\"\\n\")\n",
    "    for al in al_cum_ts:\n",
    "        f.write(str(al) + \",\")\n",
    "\n",
    "##q2\n",
    "time_diff_list = get_time_diff(cumulative_timeseries_data_list)\n",
    "# wisconsin\n",
    "wi_time_diff = time_diff_list[0]\n",
    "# alabama\n",
    "al_time_diff = time_diff_list[1]\n",
    "\n",
    "with open('q2.txt', 'w') as f:\n",
    "    for wi_diff in wi_time_diff:\n",
    "        f.write(str(wi_diff) + \",\")\n",
    "    f.write(\"\\n\")\n",
    "    for al_diff in al_time_diff:\n",
    "        f.write(str(al_diff) + \",\")\n",
    "        \n",
    "def get_beta(state_time_diff):\n",
    "    above_sum = 0\n",
    "    below_sum = 0\n",
    "    for t in range(1, len(state_time_diff)+1):\n",
    "        above_sum += (state_time_diff[t-1] - mean) * (t - (len(state_time_diff)+1)/2)\n",
    "        below_sum += np.square(t - (len(state_time_diff)+1)/2)\n",
    "    beta = above_sum/below_sum\n",
    "    return beta\n",
    "\n",
    "def get_pho(state_time_diff):\n",
    "    above_sum = 0\n",
    "    below_sum = 0\n",
    "    for t in range(2, len(state_time_diff)+1):\n",
    "        above_sum += (state_time_diff[t-1] - mean) * (state_time_diff[t-2] - mean)\n",
    "    for t in range(1, len(state_time_diff)+1): \n",
    "        below_sum += np.square(state_time_diff[t-1]-mean)\n",
    "    if below_sum != 0:\n",
    "        pho = above_sum/below_sum\n",
    "    else:\n",
    "        pho = 1\n",
    "    return pho\n",
    "\n",
    "all_cum_ts = get_cumulative_timeseries(df, all_states)\n",
    "all_time_diff = get_time_diff(all_cum_ts)\n",
    "\n",
    "means = np.zeros(num_of_states)\n",
    "stds = np.zeros(num_of_states)\n",
    "medians = np.zeros(num_of_states)\n",
    "betas = np.zeros(num_of_states)\n",
    "phos = np.zeros(num_of_states)\n",
    "\n",
    "for idx, state_time_diff in enumerate(all_time_diff):\n",
    "    mean = np.mean(state_time_diff)\n",
    "    std = np.std(state_time_diff)\n",
    "    median = np.median(state_time_diff)\n",
    "    beta = get_beta(state_time_diff)\n",
    "    pho = get_pho(state_time_diff)\n",
    "    means[idx] = mean\n",
    "    stds[idx] = std\n",
    "    medians[idx] = median\n",
    "    betas[idx] = beta\n",
    "    phos[idx] = pho\n",
    "\n",
    "# https://www.stackvidhya.com/how-to-normalize-data-between-0-and-1-range/\n",
    "def rescale(array):\n",
    "    diff = array - np.min(array)\n",
    "    max_diff = np.max(array) - np.min(array)\n",
    "    new_array = diff/max_diff\n",
    "    return new_array\n",
    "\n",
    "means = rescale(means)\n",
    "stds = rescale(stds)\n",
    "medians = rescale(medians)\n",
    "betas = rescale(betas)\n",
    "phos = rescale(phos)\n",
    "\n",
    "##q3\n",
    "params = [means, stds, medians, betas, phos]\n",
    "print(\"Mean, Standard Deviation, Medians, Betas, Phos\")\n",
    "    \n",
    "##q4\n",
    "param_matrix = np.stack(params, axis=1)\n",
    "with open('q4.txt', 'w') as f:\n",
    "    for row in param_matrix:\n",
    "        for col in row:\n",
    "            if list(row).index(col) != num_of_parameters-1:\n",
    "                f.write(str(round(col,8)) + \",\")\n",
    "            else:\n",
    "                f.write(str(round(col,8)))\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "#### HIERARCHICAL CLUSTERING\n",
    "M = param_matrix\n",
    "\n",
    "def eu_distance(x,y):\n",
    "    p=np.sum((x-y)**2)\n",
    "    d=np.sqrt(p)\n",
    "    return d\n",
    "    \n",
    "dist_matrix = np.zeros((num_of_states, num_of_states))\n",
    "for i in range(num_of_states):\n",
    "    for j in range(num_of_states):\n",
    "        if i >= j:\n",
    "            dist_matrix[i,j] = 10**10\n",
    "        else:\n",
    "            dist_matrix[i,j] = eu_distance(M[i], M[j])\n",
    "\n",
    "def single_linkage_dist(cluster1, cluster2, dist_matrix):\n",
    "    dist_list = []\n",
    "    for i in cluster1:\n",
    "        for j in cluster2:\n",
    "            if i < j:\n",
    "                dist_list.append(dist_matrix[i,j])\n",
    "            else:\n",
    "                dist_list.append(dist_matrix[j,i])\n",
    "    return min(dist_list)\n",
    "\n",
    "def complete_linkage_dist(cluster1, cluster2, dist_matrix):\n",
    "    dist_list = []\n",
    "    for i in cluster1:\n",
    "        for j in cluster2:\n",
    "            if i < j:\n",
    "                dist_list.append(dist_matrix[i,j])\n",
    "            else:\n",
    "                dist_list.append(dist_matrix[j,i])\n",
    "    return max(dist_list)\n",
    "\n",
    "def cluster_hierarchy(parameter_matrix, target_num_clusters, dist_matrix, method=\"single\"):\n",
    "    '''method should be either \"single\" or \"complete\"\n",
    "    '''\n",
    "    clusters = [[i] for i in range(len(parameter_matrix))]\n",
    "    while len(clusters) > target_num_clusters:\n",
    "        dmax = np.max(dist_matrix) + 1\n",
    "        dmin = dmax\n",
    "        dist_dic = {}\n",
    "        # clusters with minimal distances\n",
    "        min_cluster1 = None\n",
    "        min_cluster2 = None\n",
    "        for cluster1 in clusters:\n",
    "            for cluster2 in clusters:\n",
    "                if cluster1 != cluster2:\n",
    "                    if method == \"single\":\n",
    "                        dist = single_linkage_dist(cluster1, cluster2, dist_matrix)\n",
    "                    elif method == \"complete\":\n",
    "                        dist = complete_linkage_dist(cluster1, cluster2, dist_matrix)\n",
    "                    else:\n",
    "                        print(\"ERROR! METHOD should be either single or complete\")\n",
    "                    dist_dic[f'[{cluster1},{cluster2}]'] = dist\n",
    "                    if dist < dmin:\n",
    "                        dmin = dist\n",
    "                        min_cluster1 = cluster1\n",
    "                        min_cluster2 = cluster2  \n",
    "        distances = np.array(list(dist_dic.values()))\n",
    "        dmin_idxs = np.where(distances == dmin)[0]\n",
    "        cluster_pairs = list(dist_dic.keys())\n",
    "        if len(dmin_idxs) > 1:\n",
    "            # cluster pairs with the same dmin\n",
    "            cluster_pairs_with_dmin = [ast.literal_eval(cluster_pairs[i]) for i in dmin_idxs]\n",
    "            flat_list = [item for sublist in cluster_pairs_with_dmin for item in sublist]\n",
    "            min_idx = min(flat_list)\n",
    "            for cluster_pair in cluster_pairs_with_dmin:\n",
    "                if min_idx in cluster_pair:\n",
    "                    cluster_pair_with_min_idx = cluster_pair\n",
    "            min_cluster1 = cluster_pair_with_min_idx[0]\n",
    "            min_cluster2 = cluster_pair_with_min_idx[1]\n",
    "        clusters.remove(min_cluster1)\n",
    "        clusters.remove(min_cluster2)\n",
    "        clusters.append(min_cluster1 + min_cluster2)\n",
    "\n",
    "    clustering_result = []\n",
    "    for i in range(len(parameter_matrix)):\n",
    "        for c in clusters:\n",
    "            c_index = clusters.index(c)\n",
    "            #giving priority to smaller index\n",
    "            if i in c:\n",
    "                clustering_result.append(c_index)\n",
    "    return clustering_result\n",
    "\n",
    "#### SINGLE LINKAGE\n",
    "single_linkage_clustering = cluster_hierarchy(\n",
    "    M, target_num_clusters, dist_matrix, \"single\")\n",
    "\n",
    "##q5\n",
    "with open('q5.txt', 'w') as f:\n",
    "    for data in single_linkage_clustering:\n",
    "        f.write(str(data) + \",\")\n",
    "\n",
    "### COMPLETE LINKAGE\n",
    "complete_linkage_clustering = cluster_hierarchy(\n",
    "    M, target_num_clusters, dist_matrix, \"complete\")\n",
    "\n",
    "##q6\n",
    "with open('q6.txt', 'w') as f:\n",
    "    for data in complete_linkage_clustering:\n",
    "        f.write(str(data) + \",\")\n",
    "        \n",
    "### K-MEANS CLUSTERING\n",
    "\n",
    "k=target_num_clusters\n",
    "n,m = M.shape\n",
    "print(n)\n",
    "np.random.seed(2022)\n",
    "a = np.arange(n)\n",
    "np.random.shuffle(a)\n",
    "centers = M[a[:k]]\n",
    "\n",
    "def d_centers2nodes(M, centers):\n",
    "    n,m = M.shape\n",
    "    c,w = centers.shape\n",
    "    d = M.reshape([n,1,m]) - centers.reshape([1,c,w])\n",
    "    d = d**2\n",
    "    d = np.sum(d, axis=2)\n",
    "    return d\n",
    "\n",
    "for i in range(100):\n",
    "    d = d_centers2nodes(M, centers)\n",
    "    index = np.argmin(d, axis=1)\n",
    "    for j in range(k):\n",
    "        centers[j] = np.mean(M[index==j].reshape([-1,m]), axis=0)\n",
    "    #print(index)\n",
    "\n",
    "##q7\n",
    "with open('q7.txt', 'w') as f:\n",
    "    for num in index:\n",
    "        f.write(str(num) + \",\")\n",
    "\n",
    "centers = centers.round(decimals=4)\n",
    "\n",
    "##q8\n",
    "with open('q8.txt', 'w') as f:\n",
    "    for center in centers:\n",
    "        for arr in center:\n",
    "            f.write(str(arr) + \",\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "d = d_centers2nodes(M, centers)\n",
    "index = np.argmin(d, axis=1)\n",
    "distortion = 0\n",
    "for j in range(k):\n",
    "    distortion += np.sum(d[index==j,j])\n",
    "    \n",
    "##q9\n",
    "print(distortion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
